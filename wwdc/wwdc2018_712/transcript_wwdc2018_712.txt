   Hey, everyone.  Thank you. Good afternoon, and welcome to  our session on Turi Create. This session builds upon  yesterday's session on Create  ML. That session covered many of the  foundations of machine learning  in Swift. So if you didn't catch that  session I do recommend you add  it to your watch list. The goal of Turi Create is to  help you add intelligent user  experiences to your apps. For example, you might want to  be able to take a picture of  your breakfast. And tap on the different food  items to see how many calories  you're consuming. Or maybe you want to control a  lightbulb using your iPhone and  simple gestures. Maybe you want to track an  object in real time like a dog  or a picture of a dog if you  don't have dogs in the office. Maybe you have custom avatars in  your game, and you want to  provide personalized  recommendations like hairstyles  based on the beard that the user  has selected. Or maybe you want to let your  users apply artistic styles or  filters to their own photos. These are very different user  experiences. But they have several things in  common. First and foremost, they use  machine learning. These experiences all require  very little data to create. All these models were made with  Turi Create and deployed with  Core ML.

嘿，大家好。谢谢您。下午好，欢迎来到我们在Turi创建的会议。本课程以昨天关于创建ML的会议为基础。那次会议涵盖了Swift中机器学习的许多基础。所以，如果你没有赶上会议，我建议你把它添加到你的观察名单。TURI创建的目标是帮助您向应用程序添加智能用户体验。例如，你可能想拍一张你早餐的照片。点击不同的食物，看看你消耗了多少卡路里。或者你想用iPhone和简单的手势来控制灯泡。也许你想实时跟踪一个物体像狗或狗的图片，如果你没有狗在办公室。也许你的游戏里有定制的化身，你想根据用户选择的胡子提供个性化的推荐，比如发型。或者你想让你的用户把艺术风格或过滤器应用到他们自己的照片上。这是非常不同的用户体验。但它们有几个共同点。首先，他们使用机器学习。这些经验都需要很少的数据来创建。所有这些模型都是由TURI创建和部署与核心ML。

 They all follow the five, the  same five-step recipe that we'll  go over today. And all these demo apps will  also be available in our labs. So please join us today or  Friday for the ML labs to get  hands-on experience. Turi Create is a Python package  that helps you create Core ML  models. It's easy to use, so you don't  need to be an ML expert or even  have a background in machine  learning to create these  exciting user experiences. We make it easy to use by  focusing on tasks first and  foremost. What we do is we abstract away  the complicated machine-learning  algorithms so you can just focus  on the user experience that  you're trying to create. Turi Create is cross platform so  you can use it both on Mac and  Linux. And it's also open source. We have a repo on GitHub and we  hope you'll visit. There's a lot of great resources  to get started. And we look forward to working  with you and the rest of the  developer community to make Turi  Create even better over time.

他们都遵循五，同样的五步食谱，我们今天要复习。所有这些演示应用程序也将在我们的实验室中获得。因此，请加入我们今天或星期五的ML实验室，以获得动手经验。TURI CREATE是一个Python软件包，可以帮助您创建核心ML模型。它很容易使用，所以你不需要成为ML专家，甚至不需要具有机器学习的背景来创建这些令人兴奋的用户体验。我们首先通过专注于任务使它易于使用。我们所做的就是把复杂的机器学习算法抽象出来，这样你就可以把注意力集中在你试图创造的用户体验上。TURI CREATE是跨平台的，因此您可以在Mac和Linux上使用它。而且它也是开源的。我们有一个关于Github的回购，我们希望你能访问。有很多很好的资源可以开始。我们期待着与您和其他开发者社区合作，使Turi能够更好地创造一段时间。

 Today we're excited to announce  that the beta release of Turi  Create 5.0 is now available. This has some powerful new  features. Like GPU acceleration. And we'll go into these features  in detail later on today. The main focus today is going to  be the five-step recipe for  creating Core ML models. I'm going to start by going over  these steps at a high level. And then we'll dive into some  demos and code. So the first step you need is to  understand the task you're  trying to accomplish. And how we refer to that task in  machine learning terms. Second, you need to understand  the type of data you need for  this task. And how much of it.

今天我们很高兴地宣布Turi创建5的beta版本现在已经发布了。这有一些强大的新特性。像GPU加速一样。今天稍后我们将详细介绍这些特性。今天的主要焦点将是创建核心ML模型的五步骤配方。我将从一个高层次的步骤开始。然后我们会深入一些演示和代码。所以你需要的第一步就是了解你要完成的任务。以及如何在机器学习术语中引用该任务。第二，您需要了解该任务所需的数据类型。多少钱呢？

 Third, you need to create your  model. Fourth, you need to evaluate  that model. That means understanding the  quality of the model and wheth--  whether it's ready for  production. And finally, when your model's  ready to deploy, it's really  easy using Core ML. Let's dig a bit deeper into each  of these steps. Turi Create lets you accomplish  a wide variety of common machine  learning tasks. And you can work with many  different types of data. For example, if you have images,  you might be interested in image  classification. Or object detection. You might want to provide  personalized recommendations for  your users. You might want to be able to  detect automatically activities  like walking or jumping jacks. You might want to understand  user sentiment given a block of  text. Or you might be interested in  more traditional machine  learning algorithms like  classification and regression. Now we know this can get really  confusing to those of you who  are new to machine learning. So we've taken a stab at making  this really easy for you in our  documentation. We begin by helping you  understand the types of tasks  that are possible and then how  we reference them in machine  learning terms. So what we can do now is revisit  those intelligent experiences  that we walked through at the  beginning of this presentation  and assign them to these machine  learning tasks. For example, if you want to  recognize different types of  flowers in photos we call that  image classification. If you want to take pictures of  your breakfast and understand  the different food objects  within them, we call them, that  object detection. If you want to apply artistic  styles to your own photos, we  call that style transfer. And if you want to recognize  gestures or motions or different  activities using sensors of  different devices, we call that  activity classification. Finally, if you want to make  personalized recommendations to  your users, we call this task  recommender systems. Now the great thing is that the  same five-step recipe we just  walked through also applies to  your code. We begin by importing Turi  Create. We proceed to load our data into  a data structure called an  SFrame. And we'll go into a bit more  detail on the SFrame data  structure shortly. We'll proceed to create our  model with a simple function,  .create. This, this function extracts  away the complicated machine  learning behind the scenes. We proceed to evaluate our model  with the simple function  .evaluate. And finally we can export the  resulting model to Core ML's  ML-model format to be easily  dragged and dropped into Xcode. Now I mentioned that this same  five-step template applies to  all the different tasks within  Turi Create. So whether you're working on  object detection, image  classification or activity  classification, the same code  template applies. For our first demo today, we're  going to walk through a  calorie-counting app that uses  an object detection model.

第三，你需要创建你的模型。第四，你需要对模型进行评估。这意味着理解模型和WHTE的质量——是否准备好生产。最后，当您的模型准备好部署时，使用Core ML真的很容易。让我们更深入地研究这些步骤中的每一个。TURI创建使您可以完成各种各样的普通机器学习任务。你可以处理很多不同类型的数据。例如，如果你有图像，你可能会对图像分类感兴趣。或物体检测。您可能希望为用户提供个性化推荐。您可能希望能够自动检测活动如步行或跳跃千斤顶。您可能希望了解给定文本块的用户情绪。或者你可能会对更传统的机器学习算法感兴趣，比如分类和回归。现在我们知道这会让你对新的机器学习感到困惑。所以我们在文档中为你做了一个简单的尝试。我们首先帮助您理解可能的任务类型，然后如何用机器学习术语来引用它们。所以我们现在能做的就是重温我们在演讲开始时所经历的那些智能体验，并将它们分配给这些机器学习任务。例如，如果你想在照片中识别不同类型的花朵，我们称之为图像分类。如果你想拍下早餐的照片，了解其中的不同食物，我们称之为物体检测。如果你想把艺术风格应用到你自己的照片上，我们称它为风格转换。如果你想用不同设备的传感器识别手势、动作或不同的活动，我们称之为活动分类。最后，如果你想给你的用户个性化建议，我们称之为任务推荐系统。现在最重要的是，我们刚才走过的同样的五步配方也适用于你的代码。我们从导入Turi创始开始。我们将数据加载到一个称为sFrand的数据结构中。我们将在S帧数据结构上详细介绍一下。我们将用一个简单的函数来创建我们的模型。这样，这个函数就可以去掉场景背后复杂的机器学习。我们用简单的函数对模型进行评估。最后，我们可以将得到的模型导出到核心ML的ML模型格式，以方便地拖放到XCODE中。现在我提到，这个相同的五步模板适用于TURI CREATE中的所有不同任务。因此，不管您是从事对象检测、图像分类还是活动分类，都适用相同的代码模板。对于我们今天的第一个演示，我们将通过一个卡路里计数应用程序使用一个物体检测模型。

 So we'll want to recognize  different foods within an image. And we'll need to know where  those, where in the image those  foods are so that we could tap  on them to see the different  calorie counts. So let's take a look at the type  of data that we'll need to  create this machine learning  model. Of course we need images. And if we were just building a  simple image classifier model,  we would just need our set of  images and labels that describe  the images overall. But because we're performing  object detection, we need a bit  more information. We need to understand not just  what's in the image. But where those objects are. Now if we zoom in a bit closer  to one example, we see a red box  around a cup of coffee. And a green box around a  croissant. We call those boxes bounding  boxes and we represent those in  JSON format. With a label, and then with  coordinates x, y, width and  height. Where x and y refer to the  center of that bounding box. So it's also worth noting that  with object detection, you can  reference or detect multiple  images, multiple objects within  each image. So I mentioned that we'd be  loading our data into this  tabular data structure called an  SFrame. And in this example, we will end  up with two columns. The first column will contain  your images. The second column will contain  your annotations in JSON format. Let's or by now you're probably  wondering what is an SFrame?  So let's take a step back and,  and learn more about it. SFrame is a [inaudible] tabular  data structure.

所以我们想在图像中识别不同的食物。我们需要知道这些食物在哪里，在图像中这些食物在哪里，这样我们就可以点击它们来查看不同的卡路里含量。让我们来看看我们需要什么样的数据来创建这个机器学习模型。当然，我们需要图像。如果我们只是构建一个简单的图像分类器模型，我们就只需要一组描述整个图像的图像和标签。但是因为我们正在执行目标检测，所以我们需要更多的信息。我们需要理解的不仅仅是图像中的内容。但是那些物体在哪里。现在，如果我们放大一个例子，我们会看到一个红色的盒子在一杯咖啡的周围。和羊角面包周围的绿色盒子。我们将这些框称为包围盒，我们用JSON格式表示这些框。用一个标签，然后用坐标x，y，宽度和高度。其中x和y表示该边界框的中心。因此值得注意的是，通过目标检测，您可以参考或检测每个图像中的多个图像、多个对象。所以我提到我们将把数据加载到这个称为sFrm的表格数据结构中。在这个例子中，我们将得到两个列。第一列将包含您的图像。第二列将包含JSON格式的注释。或者现在你可能想知道什么是sFrand？因此，让我们后退一步，并进一步了解它。sFrand是一个[听不见]表格数据结构。

 And what this means is you can  create machine learning models  on your laptop. Even if you have enormous  amounts of data. SFrame allows you to perform  common data manipulation tasks. Like joining two SFrames or  filtering to specific rows or  columns of data. SFrames let you work with all  different data types. And once you have your data  loaded into an SFrame, it's easy  to visually explore and inspect  your data. Let's zoom in a bit more on  what's possible with SFrame. With our object detector  example. So after we import Turi Create,  in the case of our object  detector, we're actually going  to load two different SFrames. The first containing our  annotations, and the second  containing our images.

这意味着你可以在你的笔记本电脑上创建机器学习模型。即使你有大量的数据。sFrice允许您执行常见的数据操作任务。比如加入两个S帧或过滤特定的行或数据列。sFrices允许您处理所有不同的数据类型。一旦将数据加载到sFrm中，就可以很容易地可视化地检查和检查数据。让我们更详细地谈谈SFrame的可能。用我们的对象检测器例子。因此，在导入Turi创建之后，在我们的对象检测器的情况下，我们实际上要加载两个不同的SFrames。第一个包含我们的注释，第二个包含我们的图像。

 We have a simple function  .explore that will allow you to  visually inspect the data you've  imported. We can do things like access  specific rows. Or columns of our data. And of course we can do common  operations like joining our two  SFrames into one. And saving the resulting SFrame  for later use or to share with a  colleague. Next we create our model. So I mentioned we have this  simple function .create that  does all the heavy lifting for  the creation of the actual  model. And what we do behind the scenes  is we ensure that the model we  create for you is customized to  the task and that it's state of  the art. Meaning it's as high quality and  high accuracy as we can get. And we're able to do this  whether you have large amounts  of data or small amounts of  data. It's very important for us that  all of our tasks work whether  you have even a small amount of  data as small as about 40 images  per item that you're trying to  detect in the class of object. In the case of object detection. Let's move on to evaluation. So I mentioned we have a simple  function .evaluate that will  give you an idea of the quality  of your model.

我们有一个简单的函数。探索可以让你直观地检查你导入的数据。我们可以访问特定的行。或者我们数据的列。当然，我们可以做普通的操作，比如把两个S帧连接成一个。并保存生成的sFrm以供以后使用或与同事共享。接下来我们创建我们的模型。所以我提到我们有这个简单的函数。创建所有的重物来创建实际的模型。我们在幕后所做的就是确保我们为您创建的模型是根据任务定制的，并且是最先进的。这意味着我们可以获得高质量和高精确度。我们可以做到这一点，无论你有大量的数据或少量的数据。对于我们来说，我们所有的任务都很重要，不管你试图在对象类中检测到的每个项目中是否有小到大约40幅图像的少量数据。在物体检测的情况下。让我们继续进行评估。因此，我提到我们有一个简单的函数。评估将使您了解模型的质量。

 In the case of object detectors. We have two factors to consider. First, we want to know did we  get the label right?  But we also have to know if we  got that bounding box right  around the object. So we can establish a simple  metric with these two factors  and go through a test data set  scoring predictions against  known what we call ground-truth  data. And so we want to make sure that  we have correct labels. And then a standard metric is at  least 50% overlap in the  predicted bounding box when  compared with to the  ground-truth bounding box. Let's look at a few examples. In this prediction we see that  the model got the label right  with a cup of coffee. But that bounding box is not  really covering the whole cup of  coffee. It's only about ten percent  overlapping the ground truth. So we're going to consider that  a bad prediction. Here we see a highly accurate  bounding box, but we got the  label wrong. That's not a banana. So let's not consider that a  successful prediction either. Now this middle example's what  we want to see.

在对象探测器的情况下。我们有两个因素要考虑。首先，我们想知道标签是否正确？但是我们也必须知道我们是否在物体周围得到了包围盒。所以我们可以利用这两个因素建立一个简单的度量，并根据我们所谓的基础事实数据通过测试数据集对预测进行评分。所以我们要确保我们有正确的标签。然后，当与基本真值边界框相比，标准度量在预测边界框中至少有50%的重叠。让我们来看几个例子。在这一预测中，我们看到模特用一杯咖啡得到了正确的标签。但是那个包袱并没有覆盖整个咖啡。它只与地面真相重叠了百分之十。所以我们会认为这是一个糟糕的预测。这里我们看到了一个非常精确的边界框，但是我们把标签标错了。那不是香蕉。所以我们也不要认为这是一个成功的预测。现在这个中间例子就是我们想要看到的。

 We have 70% overlap of our  bounding box, and the correct  label, coffee. So what we can do is  systematically go through all of  our predictions with a test data  set. And get an overall accuracy  score for a new model. And finally, we move to  deployment. We have an export to Core ML  function that saves your model  to Core ML's ML model format. So you can then drag and drop  that model in to Xcode. This week we've actually some  exciting new features related to  object detection specifically. So I encourage you to attend  tomorrow's Vision with Core ML  session to learn more. In that session, the speaker  will actually take the object  detection model that we're  building today and go into more  detail about deployment options. And there you have it!  The five-step recipe for Turi  Create. Thank you. So with that, I'm going to hand  off to my colleague Zach Nation,  for a demo.  Thanks, Aaron. I think let's just jump straight  into code.

我们有70%个重叠的边界框，正确的标签，咖啡。所以我们可以用一个测试数据集系统地完成我们所有的预测。并获得一个新模型的总体精度得分。最后，我们转向部署。我们有一个导出到核心的ML函数，将您的模型保存到核心ML的ML模型格式。然后，您可以将该模型拖放到XCODE中。本周，我们特别关注了一些与目标检测相关的令人兴奋的新特性。因此，我鼓励您参加明天的视觉与核心ML会话，以了解更多。在那个会话中，演讲者将实际采用我们今天构建的对象检测模型，并深入讨论部署选项的更多细节。你就知道了！TURI创造的五个步骤。谢谢您。所以，我要把它交给我的同事Zach Nation，演示一下。谢谢，亚伦。我想让我们直接跳进代码。

 Who wants to write some code  live today?  We're going to go ahead and  build an object detector model  right now. So I'm going to start out in  Finder. Here I've got a folder of images  that I want to use to train a  model. We can see this folder's named  Data, and it's full of images of  breakfast foods. I've got a croissant, some eggs,  and so on. This is, this is a good data set  for breakfast food. I think let's go ahead and write  some code with it. I'm going to switch over to an  environment called Jupyter  notebook. This is an interactive Python  environment where you can run  snippets of Python code and  immediately see the output. So this is a great way to  interactively work with a model. And it's very similar in concept  to Xcode Playgrounds. The first thing we're going to  do is import Turi Create as TC. And that we way we can refer to  it as TC throughout the rest of  the script. Now, the first task we want to  do is load data. And we're going to load it up  into SFrame format. So first we can say images  equals TC.loadimages. And we're going to give it that  folder day that I just showed in  Finder. And Turi Create provides  utilities to interactively  explore and visualize our data. So let's make sure those images  loaded correctly and we got the  resulting SFrame that we  expected. I'm just going to call .explore,  and this is going to open up a  visualization window where we  can see that we have two columns  in our SFrame. The first is called path, and  it's the relative path to that  image on disk. And the second column is called  image. And that's actually the contents  of the image itself.

今天谁想写一些代码？现在我们将着手构建一个对象探测器模型。所以我要从取景器开始。在这里，我有一个图像文件夹，我想用它来训练模型。我们可以看到这个文件夹的命名数据，它充满了早餐食品的图像。我有羊角面包，一些鸡蛋，等等。这是一个很好的早餐食品数据集。我想我们继续写一些代码吧。我将切换到一个叫做Jujyter笔记本的环境。这是一个交互式Python环境，可以运行Python代码的片段，并立即看到输出。所以这是一个与模型交互工作的好方法。它在概念上与XCODE游乐场非常相似。我们要做的第一件事是导入Turi创建为TC。我们可以在整个脚本中引用它作为TC。现在，我们要做的第一个任务是加载数据。我们将把它装入sFrm格式。所以首先我们可以说图像等于TC.我们会给我一个文件夹，我刚在F取取器上展示过。Turi Cube提供实用工具来交互式地探索和可视化我们的数据。所以，让我们确保这些图片加载正确，并且我们得到了我们预期的S帧。我将调用..，这将打开一个可视化窗口，我们可以看到SFrame中有两列。第一个被称为路径，它是磁盘上图像的相对路径。第二列称为图像。这实际上是图像本身的内容。

 And we can see our breakfast  foods right here. It looks like these loaded  correctly, so I'm going to  proceed. Back in Jupyter notebook. Now I'm also going to load up a  second SFrame called  annotations. And this I'm just going to call  the SFrame instructor and  provide a file name to  annotations.csv. This is a CSV file containing  the annotations that correspond  to those images. And let's take a look at that. Right in Jupyter notebook, we  can see that this SFrame  contains a path column, again  pointing to that relative path  on disk of the image. And an annotation column  containing a JSON object  describing the bounding box and  labels associated with that  image. But now we have two different  data sources and we need to  provide one data source to train  our model. Let's join them together. In Turi Create, this is as easy  as calling the join method. I'm going to say data equals  images.joinannotations and now  we can see we have a single  SFrame with three columns. It joined on that path column. So for each image with a path,  it combined the annotations for  that path.

我们可以在这里看到早餐食品。看起来这些都是正确加载的，所以我将继续进行。回到Jupyter笔记本。现在我还要加载一个称为注释的第二个S帧。这是我要调用sFrand教官，并提供一个文件名为ANNECTIOS.CSV。这是一个包含对应于这些图像的注释的CSV文件。让我们来看一看。在Jupyter笔记本中，我们可以看到这个SFrame包含一个路径列，再次指向图像的磁盘上的相对路径。注释列包含一个JSON对象，描述边界框和与该图像相关联的标签。但是现在我们有两个不同的数据源，我们需要提供一个数据源来训练我们的模型。让我们一起去吧。在Turi创建中，这就像调用连接方法一样简单。我要说的数据等于IMAGIONION。现在我们可以看到我们有一个具有三列的S帧。它加入了那个路径列。因此，对于具有路径的每个图像，它将该路径的注释组合起来。

 And so now for each image, we  have annotations available. Now we're ready to train a  model. So I'm going to create a new  section here called train a  model. And that's just one line of code  here. I'm going to say model equals  TC.objectdetector.create. And this is our simple  task-focused API for object  detection that expects data in  this format. I'm going to pass in that data  SFrame that I just created. And for the purposes of today's  demo, I'm going to pass another  parameter called max iterations  and normally you wouldn't need  to pass this parameter because  Turi Create will pick the  correct number of iterations for  you. Based on the data that you  provide. In this case, I'm going to say  max iterations equals one just  to give an example of what  training would look like. And the reason this is going to  take a minute is it actually  goes through and resizes all of  those images in order to get  them ready to run through the  neural network. That is under the hood of this  object detector. And then it will perform just  one iteration on this Mac GPO. But this is probably not the  best model we could get because  I just wanted to train it in a  couple of seconds.

现在，对于每个图像，我们都有注释。现在我们准备训练一个模型。所以我要在这里创建一个新的部分，叫做火车A模型。这只是一行代码。我要说模型等于TC.ObjistDebug。这是一个简单的任务集中的API，用于对象检测，期望这种格式的数据。我要传递我刚刚创建的那个数据帧。对于今天的演示，我将传递另一个称为max迭代的参数，通常您不需要传递这个参数，因为Turi Create将为您选择正确的迭代次数。基于您提供的数据。在这种情况下，我会说max迭代等于一个，只是给出一个什么样的训练的例子。之所以要花一分钟，是因为它实际上要遍历并调整所有这些图像的大小，以便使它们准备好通过神经网络运行。这是在这个物体探测器的引擎盖下面。然后它将在这个MAC GPO上执行一次迭代。但这可能不是我们能得到的最好的模型，因为我只想在几秒钟内训练它。

 So I'm going to go ahead and  switch over to like cooking show  mode. And I'm going to take one out of  the oven that we've had in there  for an hour [laughter].  So I'm going to say  TC.loadmodel and it's called  breakfastmodel.model. And this is one that I've had an  opportunity to train for a bit  longer. So let's inspect that right here  in the notebook. And we can see that it's an  object detector model. It's been trained on six  classes, and we trained it for  55 minutes. This is means, this means you  can train a useful  object-detector model in under  an hour on your Mac. Next, let's test the predictions  of this model and see if it's  any good. So I'm going to make a new  section here called inspect  predictions. And we're going to go ahead and  load up a test data set. And here I've already prepared  one in SFrame format.

所以我要继续做，转向烹饪烹饪模式。我要从烤箱里拿出一个，我们已经在那里呆了一个小时[笑声]。所以我要说TC.loadmodel，它叫做RealFaskMult.Mod。这是我有机会再训练一段时间的机会。让我们在笔记本上检查一下。我们可以看到它是一个物体探测器模型。它被训练了六个班，我们训练了55分钟。这意味着，你可以在你的Mac下一小时内训练一个有用的目标探测器模型。接下来，让我们测试这个模型的预测，看看它是否好。所以我要做一个新的部分，叫做检查预测。我们将继续加载一个测试数据集。这里我已经准备了一个sFrm格式。

 So I'm just going to load it,  and I called it  testbreakfastdata.sframe. There are two important  properties of this test SFrame. One is that it contains the same  types of images that the model  would have trained on. But the second important  property is the model has never  seen these images before. So this is a good test for  whether that model can  generalize to users' real data. I'm going to make predictions  from that whole test set by  calling model.predict and  providing that test SFrame. And we'll get a batch prediction  for the whole SFrame. And that'll just take a few  seconds. And then we're going to inspect. I'm just going to pick a random  prediction here. Let's say index two. Here we can see the JSON object  that was predicted in just the  same format that the training  data is provided in. So here we have coordinates,  height, width, x and y. And a label, banana. And we get a confidence score  from the model. In this case about .87.

所以我要加载它，我称之为TestBrdFractDATA.SrFrm。这个测试框架有两个重要的特性。一个是它包含了模型所训练的相同类型的图像。但第二个重要特性是模型以前从未见过这些图像。因此，这是一个很好的测试，该模型是否可以推广到用户的真实数据。我将通过调用模型、预测和提供测试sFrand进行整个测试集的预测。我们将得到整个S帧的批量预测。这只需要几秒钟。然后我们去检查一下。我只是在这里选择一个随机预测。让我们来谈谈索引二。在这里，我们可以看到JSON对象的预测格式与训练数据提供的格式相同。这里有座标、高度、宽度、X和Y，还有一个标签，香蕉。我们从模型中得到一个置信分数。在这种情况下，大约87。

 This is a little bit hard for me  as a human to interpret though. I can't really tell if this  image is really supposed to be a  banana or whether these  coordinates are where the banana  would appear in that image. Turi Create produces a function  to take the predicted bounding  boxes or the ground-truth  bounding boxes and draw them  right onto the images. So let's go and do that. I'm going to create a new column  in my test SFrame called  predicted image. And I'm going to assign it the  output of the object detector  utility called draw bounding  boxes. And I'm going to pass into draw  bounding boxes that test image  column. So that's the image itself and  then I'm also going to pass the  predictions that I just got from  the model. That's going to draw those  predicted bounding boxes onto  each image. Now let's take a look at that  number two prediction again. This time in image form. So I can say  testpredictedimage2.show. And it will render right here in  the notebook. And this is great as a spot  check because at least for one  picture, we know that the  model's working.

这对我来说是有点难理解的。我真的不知道这个图像是否真的应该是香蕉，或者这些坐标是否就是香蕉在图像中出现的位置。Turi Create产生一个函数来获取预测的边界框或基本事实边界框，并将它们直接绘制到图像上。让我们去做吧。我将在我的测试框架中创建一个叫做预测图像的新列。我将把对象检测器实用程序的输出指定为绘制边界框。我会进入测试图像列的绘制边界框。这就是图像本身，然后我也会通过我刚刚从模型中得到的预测。这将把预测的边界框画在每个图像上。现在让我们再来看看第二个预测。这一次以图像的形式出现。所以我可以说TestRealTimeDima2.它会在笔记本上呈现。这是一个很好的抽查，因为至少对于一张图片，我们知道模型是有效的。

 But this doesn't tell us if  it'll work for say the next  50,000 images that we pass in. So for that, we're going to  evaluate the model  quantitatively. And I'm going to start a new  section here in the notebook  called evaluate the model. And what we're going to do is  call model.evaluate and once  again, I'm going to pass in just  that whole test data set. Here the evaluation function is  going to run the metric that  Aaron described, testing whether  the bounding boxes are  overlapping at least 50% and  have a correct label. And it's going to give us that  result across each of the six  classes that we trained on. So here we can see that our  overlapping bounding boxes with  the correct label are happening  about 80% of the time for bagel. About 67% of the time for  banana. And so on. That's pretty good, so I think  let's, let's see if this model's  actually going to work in a real  app. I'm going to go ahead and call  exportcoreml to create a Core ML  model from the model we just  trained. And I'm going to call it  breakfastmodel.mlmodel.

但这并不能告诉我们接下来的50000张图片是否会起作用。因此，我们将对模型进行定量评估。我将在笔记本中创建一个新的部分，称为“评估模型”。我们要做的是调用Myto.Actudio，再一次，我将通过整个测试数据集。在这里，评估函数将运行Aaron描述的度量，测试边界框是否至少重叠50%并具有正确的标签。它将给我们在我们训练的六个班级中的每一个结果。因此，我们可以看到，我们的重叠包围盒与正确的标签发生约80%的时间面包圈。大约67%的香蕉时间。等等。这很好，所以我想让我们看看这个模型在实际应用中是否真的有效。我将继续调用ExtCopeML，从我们刚刚训练过的模型中创建一个核心ML模型。我将称之为RealFaskMult.MLMead。

 And then as soon as that's done  training, I'm going to go ahead  and open it in finder. Or sorry. As soon as that's done  exporting. So here in finder, I've got my  breakfastmodel.mlmodel. And when I open it in Xcode, I  can see that it looks just like  any Core ML model. It takes an input image, and as  output we get confidence and  coordinates. And that's going to tell us the  predicted bounding box and label  for the image that we have. Now let's switch over to the  iPhone app where we're going to  consume this model. So here on my iPhone, I've got  an app called Food Predictor. And this is going to use the  model that we just trained. Here I'm going to choose from  photos. And I've got a picture of this  morning's breakfast. This is a pretty typical  breakfast for me: coffee and a  banana. Well, often I skip the banana. But suppose I ate a banana this  morning.

然后，一旦完成训练，我将继续在F取取器中打开它。或者抱歉。一旦出口完成。所以在F取取器中，我得到了我的Rebug模型。当我在XCODE中打开它时，我可以看到它看起来像任何内核ML模型。它需要输入图像，作为输出，我们得到信心和坐标。这将告诉我们我们所拥有的图像的预测边界框和标签。现在让我们切换到iPhone应用程序，在那里我们将使用这个模型。所以在我的iPhone上，我有一个叫做“食物预测器”的应用程序。这将使用我们刚刚训练过的模型。这里我将从照片中选择。我有一张今天早上早餐的照片。对我来说，这是非常典型的早餐：咖啡和香蕉。嗯，我经常跳过香蕉。但是假设我今天早上吃了一个香蕉。

 We can just tap right on the  image. And because we know the bounding  box, we can identify the object  within that bounding box. And here we see the model tells  us this is a banana, and this is  a cup of coffee. So let's recap what we just saw. First, we loaded images and  annotations into SFrame format  and joined them together with a  simple function call. We interactively explored that  data using the explore method. We created a model just with a  simple high-level API< passing  in that data object containing  both the images and the bounding  boxes and labels. We then evaluated that model  both qualitatively,  spot-checking the output as a  human would. And quantitatively, asking for a  specific metric that applies to  the task that we're doing. Then we exported that model to  Core ML format for use in an  app. Next, I'd like to switch gears  and talk about some exciting new  features in Turi Create 5.0. Turi Create 5.0 has a new task  called style transfer. We have major performance  improvements from native GPU  acceleration on your Mac. And we have new deployment  options including recommender  models for personalization and  vision feature print-powered  models so that you can reduce  the size of your app. Taking advantage of models that  are already in the operating  system.

我们可以直接点击图像。因为我们知道边界框，所以我们可以识别这个边界框中的对象。我们看到这个模型告诉我们这是一个香蕉，这是一杯咖啡。让我们回顾一下我们刚刚看到的。首先，我们将图像和注释加载到sFrand格式中，并用简单的函数调用将它们结合在一起。我们采用探索法对数据进行交互探索。我们仅用一个简单的高级API<传入包含图像和边界框和标签的数据对象，就创建了一个模型。然后，我们定性地评估模型，即刻检查输出作为人类意志。定量地，要求一个具体的度量，适用于我们正在做的任务。然后，我们将该模型导出到应用程序中使用的核心ML格式。接下来，我想切换齿轮和谈论一些令人兴奋的新特点在Turi创建5。TURI CREATE 5有一个新的任务称为风格转移。我们有重大的性能改进从本地GPU加速在您的Mac上。我们还有新的部署选项，包括个性化的推荐模型和基于视觉特性的打印模型，以便减少应用程序的大小。利用已经在操作系统中的模型。

 Let's talk a little bit more  about that style transfer task. Imagine we've got some style  images and these are really cool  looking recognizable stylistic  images. Here we've got sort of a light  honeycomb pattern and a very  colorful flower pattern. And we want apply those as  filters to our own images that  we take with a camera. We've got a dog photo here and  what it would look like to apply  those styles to that dog is  something like that. And with a style transfer model,  we can take the same styles and  apply them to more photos. Let's say a cat and another dog. And that's the sort of effect we  would get. Here's an example of an app that  uses style transfer for filters  on photos that a user would  take. The code to create the style  transfer model follows the same  five-step recipe as any other  high-level task in Turi Create. So you can start by importing  Turi Create, loading data into  the SFrame format, creating the  model with a simple high-level  API. Then we make predictions, in  this case with a function called  stylize to take an image and  apply that style filter.

让我们再谈一点关于风格转移的任务。想象一下，我们有一些风格的图像，这些都是很酷的识别风格的图像。这里我们有一种浅蜂窝图案和一种非常花花绿绿的花纹。我们希望把这些滤光片应用到我们自己的相机上。我们这里有一张狗的照片，把这些样式应用于那只狗会是什么样子。采用风格转换模型，我们可以采取相同的风格，并将它们应用于更多的照片。比如说猫和另一只狗。这就是我们能得到的效果。下面是一个应用程序的例子，该程序使用样式转换来为用户拍摄的照片提供过滤器。创建样式转换模型的代码遵循与Turi中创建的任何其他高级任务相同的五步骤配方。因此，您可以从导入Turi Create、将数据加载到SFrame格式、使用简单的高级API创建模型开始。然后我们进行预测，在这种情况下，调用一个名为SytLIZE的函数来获取一个图像并应用该样式过滤器。





 So in this case, it's just a  variety of photographs. We're going to load a folder  called content into an SFrame  for that one. And then we're ready to go ahead  and train a model. So I'm going to say model equals  tc.styletransfer.create. And I'm going to pass in style  and content and that's all we  need. But that's going to take a bit  too long to train for today's  demo. So once again, I'm going to do  it like a cooking show, and  we're going to load up one that  we've had in the oven already. I'm going to say model equals  tc.loadmodel and I'm going to  load up my already trained style  transfer model. Let's take a look at some of  these style images to see what  we should expect this model to  produce. We have a style image col-- we  have an image column in our  style SFrame.

所以在这种情况下，它只是各种各样的照片。我们将把一个名为内容的文件夹加载到一个S帧中。然后我们准备继续训练一个模型。所以我要说模型等于TC.StaselTrime.Cube。我要通过风格和内容，这就是我们所需要的。但这将花太长时间来训练今天的演示。所以，我要再做一个烹饪节目，我们要装一个我们已经在烤箱里吃过的。我要说模型等于TC.Load模型，我将加载我已经训练过的风格传输模型。让我们来看看这些样式图像，看看我们应该期待这个模型产生什么。我们有一个风格图像COLL -我们有一个图像列在我们的风格SFrame。

 And let's take a look at just  style number three and see what  that looks like. It-- sort of like a pile of  firewood. And this is pretty stylistic. I think this would be  recognizable if we were to apply  it as a filter to another image. Now let's take a look at some  content images. I'm going to load up a test data  set. And once again this is a data  set that is representative of  the types of images that users  will have at runtime in your  app. And the important thing is that  the model never saw these at  training time.

让我们看一下样式三，看看它是什么样子。它有点像一堆柴火。这很有风格。我认为如果我们把它作为一个过滤器应用到另一个图像，这是可以识别的。现在让我们来看看一些内容图像。我要装入一个测试数据集。再一次，这是一个数据集，它代表用户在运行时在应用程序中拥有的图像类型。最重要的是，这个模型在训练时从来没有见过。

 So by evaluating with the test  images, we'll know whether the  model can generalize to users'  data. I'm going to load up a test data  set now. With tc.loadimages function once  again. And we're going to call that  folder test. And I'm going to pull out one  image from the test data set  called ample image. And I'm just going to take the  first image there. And I'm going to call .show. So that we can we see what that  image looks like without any  filters applied. That's my cat, seven of nine. She always looks like that. And we're going to go ahead and  stylize that image using the  model that we just trained. So I'm going to say stylized  image equals model.stylize. And in this case, the function  is called stylize because the  model is specific to the task of  style transfer.

因此，通过对测试图像进行评估，我们将知道模型是否可以推广到用户的数据。我现在要加载一个测试数据集。用TC.LoaDimes再次功能。我们将称之为文件夹测试。我将从被称为“充足图像”的测试数据集中取出一个图像。我要拍第一张照片。我要打电话给节目。这样我们就可以看到没有任何过滤器的图像是什么样子。那是我的猫，九只猫中的七只。她看起来总是那样。我们将继续使用我们刚刚训练过的模型来塑造图像。所以我要说程式化的图像等于模特。在这种情况下，该函数被称为风格化，因为该模型是特定于风格转移的任务。

 And we're going to pass in that  sample image. And I'm going to say style  equals three because that's the  style that we picked earlier  that looks like firewood here. So let's see what that stylized  image looks like. I can call .show on that, and  here is my cat looking like a  pile of firewood. Let's make sure this works on  other styles, too. I'm going to go ahead and make a  stylized image out of that  sample image. And I'm going to specify a style  equals seven. And then let's see what that  looks like. That looks pretty good. I wonder what the style was that  we just applied to that. Let's take a look at style  images. Style image seven. And once again we can just call  .show to see what that style  image looks like and yeah, that  looks like the filter that we  just applied to my cat. Now that we've got a good style  transfer model, we can just call  model.exportcoreml exactly the  same as any other model, and  save it into Core ML format. Now, let's switch over to the  iPhone where we have a style  transfer app ready to apply the  filters in this model. So here I've got my iPhone once  again. And I have an app called style  transfer.

我们将通过那个样本图像。我会说风格等于三，因为这是我们之前挑选的风格，这里看起来像柴火。让我们看看这个程式化的图像是什么样子的。我可以打电话给你看，这是我的猫看起来像一堆柴火。让我们确保这也适用于其他风格。我要先从那个样本图像中做一个程式化的图像。我将指定一个样式等于七。然后让我们看看这是什么样子。看起来不错。我想知道我们刚才用的是什么风格。让我们来看看风格图像。样式图像七。再一次，我们可以调用.show来查看这个样式的图像是什么样子，是的，它看起来就像我们刚刚应用到我的猫身上的过滤器。既然我们已经有了一个好的样式转换模型，我们可以调用model.exportcoreml和其他任何模型完全一样，并将其保存为Core ML格式。现在，让我们切换到iPhone，在那里我们有一个风格转移应用程序准备在这个模型中应用过滤器。所以我又有了iPhone。我有一个叫做风格转移的应用程序。

 I'm going to choose a photo from  my photo library to apply these  styles to. These are my dogs. This is Ryker and we're going to  see what styles we have  available in this app. We can scroll through all of the  styles here and what's important  to note is that a single style  transfer model was trained on  all of these files. And one model can include any  number of styles. So to have multiple filters, you  don't need to greatly increase  the size of your app. Let's see what those styles look  like applied to Ryker. Pretty cool. So--  So to recap what we just saw, we  loaded images into SFrame  format. This time style and content  images into two SFrames. We created a model using a  high-level API for style  transfer that operates directly  on a set of style images and a  set of content images. We then stylize images to check  whether the model is performing  well. We visualized those predictions  in Turi Create. And finally we exported the  model in Core ML format for use  in our app. Switching gears a bit. I want to talk about some other  features in Turi Create 5.0. We now have Mac GPU acceleration  offering up to a 12x performance  increase in image  classification. And 9x in object detection, and  that's on an iMac Pro. We have a new task available for  export into Core ML format. Personalization. The task here is to recommend  items for users based on user's  historical preferences. This type of model is deployed  using Core ML's new custom model  support that's available on  macOS Mojave and on iOS 12. This has been a top community  feature request since we open  sourced Turi Create.

我要从照片库中选择一张照片来应用这些样式。这些是我的狗。这是Ryker，我们将看到什么样的风格，我们可以在这个应用程序。我们可以在这里浏览所有样式，需要注意的是，在所有这些文件上都训练了单个样式的传输模型。一个模型可以包括任意数量的样式。因此，有多个过滤器，你不需要大大增加你的应用程序的大小。让我们看看这些样式看起来如何应用于Ryker。相当酷。因此，为了重新审视我们刚才看到的，我们将图像加载到sFrand格式。这一时间风格和内容成两个画面。我们使用一个用于样式转移的高级API创建了一个模型，该模型直接对一组样式图像和一组内容图像进行操作。然后，我们风格化图像，以检查模型是否表现良好。我们想象Turi创造的那些预言。最后，我们以核心ML格式导出了模型，用于我们的应用程序。换挡一点。我想谈谈Turi创造5的一些其他特征。我们现在有MAC GPU加速提供高达12x的性能提高图像分类。和9X在物体检测，这是一个IMAC专业。我们有一个新的任务可导出到核心ML格式。个人化。这里的任务是根据用户的历史偏好为用户推荐项目。这种类型的模型部署使用MyML的新的自定义模型支持，可以在MaOS Mjjaveand iOS 12上使用。这是一个顶级社区的特点要求，因为我们开源Turi创建。

 So I'm really excited to bring  it to you today. The recommender model in Core ML  looks just like any other Core  ML model. But what's worth noting is  there's a section at the bottom  here called Dependencies. And in this section, you can see  that this model uses a custom  model and that model is called  TC recommender. And this is just Turi Create  providing support for  recommenders in Core ML through  that custom model API. Using that model in Core ML look  very similar to any other Core  ML model as well. You can just instantiate the  model, create your input. So in this case, we've got that  avatar creation app. And a user might have picked a  brown beard and a brown  handlebar moustache and brown  long hair for their avatar. And we can make predictions from  the model. By providing those interactions  as input. And where we say k10 means we'll  get the top ten predictions  given those inputs. So to recap what we've learned  today. Turi Create allows you to create  Core ML models to power  intelligent features in your  apps. It uses a simple five-step  recipe starting with identifying  the task that you're doing. And mapping it to a machine  learning task. Gathering and annotating data  for use in training that model.

所以今天我很兴奋把它带给你。核心ML中的推荐模型与任何其他核心ML模型一样。但是值得注意的是底部有一个部分叫做依赖关系。在本节中，您可以看到该模型使用自定义模型，该模型称为TC推荐器。这就是TURI创建，通过自定义模型API为核心ML中的推荐者提供支持。使用该模型在核心ML看起来非常类似于任何其他核心ML模型。你可以实例化模型，创建你的输入。在这种情况下，我们有了化身制作应用程序。一个用户可能为他们的化身挑选了一个棕色胡须和一个棕色的车把胡子和棕色的长发。我们可以从模型中做出预测。通过提供这些交互作为输入。我们说K10意味着我们将得到十个预测。所以来回顾一下我们今天学到的东西。TURI CREATE允许您创建核心ML模型来为应用程序中的智能特性提供电源。它使用一个简单的五步配方开始识别你正在做的任务。并将其映射到机器学习任务。收集和注释数据以用于训练该模型。

 Training the model itself using  a simple, high-level API  specific to the task that you're  doing. Evaluating that model in Turi  Create both qualitatively and  quantitatively. And finally, deploying in Core  ML format. That five-step recipe maps to  code starting with import Turi  Create. You can load data into the  SFrame format. Create a model using that  task-specific API. Evaluate the model with an  evaluate function that's once  again specific to the task that  you're doing. And export for deployment,  calling the export Core ML  function. Turi Create supports a broad  variety of machine learning  tasks. Ranging from high-level tasks  like image classification and  text classification, all the way  to low-level machine learning  essentials. Like regression and  classification on any type of  data. And using the resulting models,  you can power intelligent  features in your apps like  object detection or style  transfer for use as a filter. For more information, please see  the Developer.Apple.com session  URL. And please come to our labs. We've got a lab this afternoon  and Friday afternoon.

使用一个简单的、高级的API来训练模型本身。评价Turi的模型既有定性又有定量。最后，以内核ML格式部署。五步配方映射导入导入TURI创建的代码。可以将数据加载到sFrm格式中。使用特定于任务的API创建模型。用一个评估函数来重新评估模型，它再次对你所做的任务有特殊性。并导出用于部署，调用导出核心ML函数。TURI CREATE支持各种各样的机器学习任务。从图像分类和文本分类等高级任务一直到低级机器学习要点。类似于任何类型的数据的回归和分类。使用最终的模型，你可以在你的应用程序中提供智能特性，比如对象检测或者样式转换，作为过滤器使用。欲了解更多信息，请参阅开发人员.Apple .com会话URL。请到我们的实验室来。今天下午和星期五下午我们有一个实验室。

 And we welcome your feedback. We're happy to answer any  questions you have. And we'll have all of the demos  we showed today available to  explore. Thank you.

我们欢迎你的反馈。我们很乐意回答你的任何问题。我们将展示今天展示的所有演示。谢谢您。

